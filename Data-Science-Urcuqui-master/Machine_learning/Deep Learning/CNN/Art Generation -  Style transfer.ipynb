{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Style\n",
    "\n",
    "In order to combine the content of one image with the style of another we must use a feature space designed to capture texture and color information \n",
    "\n",
    "It is exist a correleation that means a relationship between two or more variables, similarities and differences between featurs in a layer should give some information about the texture and color information found in an image. \n",
    "\n",
    "Content and style can be seperate components of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example we are going to use the features in the architecure of 19 VGG network, specifically, the output from the fourth convolutional stack (conv 4_2)\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg\" width=\"400\" />\n",
    "\n",
    "We need to comparte the content representation with our content image through a _Content Loss_  \n",
    "\n",
    "_Content Representation_ from the content image $=C_{c}$\n",
    "\n",
    "_Content Representation_ from the target image $=T_{c}$\n",
    "\n",
    "In this case we calculate the mean squared difference between two representations.\n",
    "\n",
    "$L_{content} =  \\frac{1}{2}\\sum(T_{c} - C_{c})^{2}$\n",
    "\n",
    "The last loss measures how far awa these two representations are from one to another.\n",
    "\n",
    "In the same idea we need to do this to the style representations of our target image, the style representation is calculated as an image passes through the network at the first convolutional layer in all five stacks (conv1_1, conv2_1, conv3_1. conv4_1 and conv5_1), and the correlations at each layer are given by a _Gran matrix_ that contains non-localized infomration about the layers.\n",
    "\n",
    "The first step to calculate the _gran matrix_ is to vectorize the values in the convolutional layer, for example think of one with dimensions of \n",
    "``` d x h x w = (20*8*8)``` it's spational dimensions flattened is 20x64.\n",
    "\n",
    "Once we have the previous result the dimensions of the _Gran Matrix_ is the matrix flattened (20x64) by it's transpose 64x20 result is 20x20, the previous size represents the depth of the convolutional layer. \n",
    "\n",
    "For the style representation we need to calculate the distance between:\n",
    "\n",
    "_List of Gram Matrices_ from the style image $=S_{c}$\n",
    "\n",
    "_List of Gram Matrices_ from the target image $=T_{c}$\n",
    "\n",
    "Through each layer (conv1_1, conv2_1, conv3_1. conv4_1 and conv5_1) and we are going to have a constant _a_ that represenets the number of their values.\n",
    "\n",
    "$L_{style} =  a\\sum w_{i}(T_{(s,i)} - S_{(s,i)})^{2}$\n",
    "\n",
    "So we are going to have the __total loss__, and use back propagartion and optimization to reduce this loss\n",
    "\n",
    "_Total loss_ $ =  \\alpha L_{content} + \\beta L_{style}$\n",
    "\n",
    "$\\alpha$ represents the content weights and $\\beta$ the style weights, and often it is expresed the ratio of the content weights $\\frac{\\alpha}{\\beta}$ where the more smaller the alpha-ratio, the more stylistic effect you will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nst_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-480c240b4f87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnst_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nst_utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
